---
title: "6th week: Follow-person project."
last_modified_at: 2022-09-25T20:50:00
categories:
  - Blog
tags:
  - ROS Foxy
  - Visual Circuit
youtubeId_cam_no_reliable: A6Nit431HAk
---

On the seventh week I had to face several problems.

By testing the last model I had, I found out that the outputs and inputs can only have 1 cable connected, because if you use 2 cables in the same output, the info is only shared with one of the cables and not with the other. 

![](/2022-tfg-david-tapiador/images/multicable.png)

The same thing happens with the inputs, so if you have several cables into the same input, the block only receives from one of them

![](/2022-tfg-david-tapiador/images/multicable_read.png)
![](/2022-tfg-david-tapiador/images/multicable_read_grep.png)

To solve this, we need to add another input/output, so that the info is read or shared several times.

![](/2022-tfg-david-tapiador/images/multicable_2.png)

With this in mind, I had to remake the model I had. Because if we remember the last model I had, the results from the ObjectDetector was shared into 2 different blocks, and also the MotorDriver received 2 different velocities depending on what got enabled.

![](/2022-tfg-david-tapiador/images/model_prev.png)

Now I had to change the blocks to have 1 more output/input to be able to have these multicable setup. Also the inside code had to be changed, because now insted of just reading the velocities, we have to read both and see which one is the good one.

![](/2022-tfg-david-tapiador/images/model_post.png)

Also I faced one more problem. The object detector had a real problem while trying to recognise a "person" while seeing his back.
As it is shown in the video, when the person is facing the camera, the bounding box is moreless stable and is always there. But when the person turns around (as it will be in the final program, as the program is to follow a person, so the visible part will be his back), the bounding box blinks, making it imposible to follow the person as it will change from "follow mode" to "searching-person mode" every second.

{% include youtubePlayer.html id=page.youtubeId_cam_no_reliable %}

I've been trying to change every parameter and everything in the ObjectDetector block to try and make it stable, but I've been unable to achieve it :C

This next week I'll try to make it work out with a real person or even the real robot to test if the problem is the simulated camera and the low quality of the person model, or if the problem is somewhere else ðŸ˜”






This "sixth week" (actually it has been the whole august month, but halve of it i was on vacations ^.^) I had to update the ROS2 blocks, because one of my partners of the VisualCircuit project told me to add some images and description to the blocks following a simple tutorial they did some time ago. Now all the blocks have been added to the online version of VisualCircuit!

![](/2022-tfg-david-tapiador/images/ROS2_blocks_added.png)

Also I started working on a full project to test VisualCircuit. This project is a follow-person using vision and ROS2.
To start the project I needed a scenario to deploy the robot and the person. Here I used the [hospital world](https://github.com/aws-robotics/aws-robomaker-hospital-world) that you can find on the amazon web services on github. I used the one floor world, because I'm not going to program the turtlebot to comunicate with different objects like elevators, I'm going to make it follow a person inside the hospital.

![](/2022-tfg-david-tapiador/images/hospital_world.png)

For the moving person part, I found that my friend Carlos Caminero already had a [plugin](https://github.com/RoboticsLabURJC/2021-tfg-carlos-caminero/tree/main/person_teleop) to spawn and control manually a person model, so I used his plugin here. To use it, you need to build his repository using colcon build (ros-foxy way to compile) and execute the executable file found on "(ros-foxy workspace)/install/person_teleop/bin" which is also called "person_teleop".

![](/2022-tfg-david-tapiador/images/elman.png)![](/2022-tfg-david-tapiador/images/spawned.png)

Here you can see how it moves following a path using objectives by coordinates.

{% include youtubePlayer.html id=page.youtubeId_person %}

Now, to program the robot I started thinking on the inside logic that the robot might follow.
The objective is to follow a person, but for that, we need to find the person, and if at the begining there is no person on vision, we need to rotate to find it.
So I designed some kind of decision tree with only 2 branches: follow the person or rotate to find a person.

![](/2022-tfg-david-tapiador/images/prev_model.png)

But later I saw that it was not optimal on resource use, due to the duplicated blocks for the same purpose. So I started thinking on another way of working the same way with just one cam and one object detector blocks. This is what I finally got:

![](/2022-tfg-david-tapiador/images/actual_model.png)

Here the behavior is the same, but instead of enabling the object detector of each side, we enable the rotation or follow mode.

For the next weeks, I have to adjust the PID to follow correctly the person and maybe add a laser block to mantain the distance with the human.